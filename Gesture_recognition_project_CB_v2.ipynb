{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INwYHEBPsF7h"
   },
   "source": [
    "# Gesture Recognition Project:\n",
    "\n",
    "### Problem statement: Develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBpKWW_yss1j"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1693743105418,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "uoftFXYwrIkD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize as imresize\n",
    "import datetime\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "import cv2\n",
    "\n",
    "np.random.seed(30)          # for reproducability\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout,LSTM\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D,Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3983,
     "status": "ok",
     "timestamp": 1693743109396,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "Ihtggwtrvhtn",
    "outputId": "0a98532f-dcca-44c9-9158-f030ab0abdb8"
   },
   "outputs": [],
   "source": [
    "## Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbyeKbiMuQQz"
   },
   "source": [
    "### Read the folder names for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1693743110567,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "f2cWZZdluoMT"
   },
   "outputs": [],
   "source": [
    "# np.random.permutation will randomly select all train and validation files from CSV files\n",
    "\n",
    "train_doc = np.random.permutation(open('datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPNu71pbxmxu"
   },
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Generator with grayscale color channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    \"\"\"\n",
    "    Generator function for creating batches of data for training and validation.\n",
    "\n",
    "    Args:\n",
    "        source_path: The path to the directory containing the gesture data.\n",
    "        folder_list: A list of the folders containing the gesture data (CSV files).\n",
    "        batch_size: The size of the batches to be created.\n",
    "\n",
    "    Yields:\n",
    "        A tuple of (batch_data, batch_labels), where:\n",
    "            batch_data: A numpy array of shape (batch_size, len(img_idx), 120, 120, 1) containing the grayscale image data for the batch.\n",
    "            batch_labels: A numpy array of shape (batch_size, 5) containing the one-hot encoded labels for the batch.\n",
    "    \"\"\"\n",
    "    print('Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]  # Use even-numbered frames of video\n",
    "\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)  # Randomly shuffle the order of the folders in folder_list\n",
    "        num_batches = len(folder_list) // batch_size  # Number of batches\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size, len(img_idx), 120, 120, 1))  # Grayscale images\n",
    "            batch_labels = np.zeros((batch_size, 5))  # One-hot encoded labels\n",
    "\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path + '/' + t[folder + (batch * batch_size)].split(';')[0])  # List of images in the video folder\n",
    "\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = cv2.imread(\n",
    "                        source_path + '/' + t[folder + (batch * batch_size)].strip().split(';')[0] + '/' + imgs[\n",
    "                            item])\n",
    "                    \n",
    "                    # Convert the image to grayscale for reducing the model parameters\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "                    image = cv2.resize(image, (120, 120))  # Resize the image to (120, 120)\n",
    "\n",
    "                    # Normalize the grayscale image to the range [0, 1]\n",
    "                    image = image / 255.0\n",
    "\n",
    "                    batch_data[folder, idx, :, :, 0] = image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (batch * batch_size)].strip().split(';')[2])] = 1  # One-hot encoded labels\n",
    "\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        # Handle remaining data points after full batches\n",
    "        remaining_samples = len(folder_list) % batch_size\n",
    "        print('remaining samples found', remaining_samples)\n",
    "        if remaining_samples > 0:\n",
    "            batch_data = np.zeros((batch_size, len(img_idx), 120, 120, 1))  # Grayscale images\n",
    "            batch_labels = np.zeros((batch_size, 5))\n",
    "\n",
    "            for folder in range(remaining_samples):\n",
    "                imgs = os.listdir(source_path + '/' + t[folder + (num_batches * batch_size)].split(';')[0])  # List of images in the video folder\n",
    "\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = cv2.imread(\n",
    "                        source_path + '/' + t[folder + (num_batches * batch_size)].strip().split(';')[0] + '/' + imgs[\n",
    "                            item])\n",
    "                    \n",
    "                    # Convert the image to grayscale\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "                    image = cv2.resize(image, (120, 120))  # Resize the image to (120, 120)\n",
    "\n",
    "                    # Normalize the grayscale image to the range [0, 1]\n",
    "                    image = image / 255.0\n",
    "\n",
    "                    batch_data[folder, idx, :, :, 0] = image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches * batch_size)].strip().split(';')[2])] = 1  # One-hot encoded labels\n",
    "\n",
    "            yield batch_data[:remaining_samples], batch_labels[:remaining_samples]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Generator with RGB Color channels (for transfer learning use case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_2(source_path, folder_list, batch_size):\n",
    "    \"\"\"\n",
    "    Generator function for creating batches of data for training and validation.\n",
    "\n",
    "    Args:\n",
    "        source_path: The path to the directory containing the gesture data.\n",
    "        folder_list: A list of the folders containing the gesture data. ( In our case it is CSV file)\n",
    "        batch_size: The size of the batches to be created.\n",
    "\n",
    "    Yields:\n",
    "        A tuple of (batch_data, batch_labels), where:\n",
    "            batch_data: A numpy array of shape (batch_size, x, y, z, 3) containing the image data for the batch.\n",
    "            batch_labels: A numpy array of shape (batch_size, 5) containing the one-hot encoded labels for the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]  # we will use even number frames (images) of video for model training\n",
    "    # img_idx= list(range(30))                             # we will use all images of video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)            # randomly shuffling the order of the folders in the folder_list\n",
    "        num_batches = len(folder_list) // batch_size      # number of batches\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size, len(img_idx), 120, 120, 3))   # Here, we will use total len(img_idx) images for each video with (120,120) image size\n",
    "            batch_labels = np.zeros((batch_size, 5))      # batch_labels is the one hot encoded representation of the output\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path + '/' + t[folder + (batch * batch_size)].split(';')[0]) # read all the images in the video folder\n",
    "\n",
    "                # For each image of Video foler we will normalize it and resize it to make same shape input to the Model\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = imread(source_path + '/' + t[folder + (batch * batch_size)].strip().split(';')[0] + '/' + imgs[item]).astype(np.float32) # read each image\n",
    "                    image = imresize(image, (120, 120)) # reshape image size to (120,120)\n",
    "\n",
    "                    # Use Min-Max rescaling for image normalization for each color channel\n",
    "\n",
    "                    batch_data[folder, idx, :, :, 0] = (\n",
    "                        image[:, :, 0] - np.min(image[:, :, 0])\n",
    "                    ) / (\n",
    "                        np.max(image[:, :, 0]) - np.min(image[:, :, 0])\n",
    "                    )\n",
    "                    batch_data[folder, idx, :, :, 1] = (\n",
    "                        image[:, :, 1] - np.min(image[:, :, 1])\n",
    "                    ) / (\n",
    "                        np.max(image[:, :, 1]) - np.min(image[:, :, 1])\n",
    "                    )\n",
    "                    batch_data[folder, idx, :, :, 2] = (\n",
    "                        image[:, :, 2] - np.min(image[:, :, 2])\n",
    "                    ) / (\n",
    "                        np.max(image[:, :, 2]) - np.min(image[:, :, 2])\n",
    "                    )\n",
    "                batch_labels[folder, int(t[folder + (batch * batch_size)].strip().split(';')[2])] = 1  # one-hot encoded representation of output labels\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        # Handle remaining data points which are left after full batches\n",
    "\n",
    "        remaining_samples = len(folder_list) % batch_size\n",
    "        if remaining_samples > 0:\n",
    "            batch_data = np.zeros((batch_size, len(img_idx), 120, 120, 3))   # Here, we will use total len(img_idx) images for each video with (120,120) image size\n",
    "            batch_labels = np.zeros((batch_size, 5))\n",
    "\n",
    "            for folder in range(remaining_samples):\n",
    "                imgs = os.listdir(source_path + '/' + t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the video folder\n",
    "\n",
    "                # For each image of Video foler we will normalize it and resize it to make same shape input to the Model\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = imread(source_path + '/' + t[folder + (num_batches*batch_size)].strip().split(';')[0] + '/' + imgs[item]).astype(np.float32) # read each image\n",
    "                    image = imresize(image, (120, 120)) # reshape image size to (120,120)\n",
    "\n",
    "                    # Use Min-Max rescaling for image normalization for each color channel\n",
    "\n",
    "                    batch_data[folder, idx, :, :, 0] = (\n",
    "                        image[:, :, 0] - np.min(image[:, :, 0])\n",
    "                    ) / (\n",
    "                        np.max(image[:, :, 0]) - np.min(image[:, :, 0])\n",
    "                    )\n",
    "                    batch_data[folder, idx, :, :, 1] = (\n",
    "                        image[:, :, 1] - np.min(image[:, :, 1])\n",
    "                    ) / (\n",
    "                        np.max(image[:, :, 1]) - np.min(image[:, :, 1])\n",
    "                    )\n",
    "                    batch_data[folder, idx, :, :, 2] = (\n",
    "                        image[:, :, 2] - np.min(image[:, :, 2])\n",
    "                    ) / (\n",
    "                        np.max(image[:, :, 2]) - np.min(image[:, :, 2])\n",
    "                    )\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1  # one-hot encoded representation of output labels\n",
    "            yield batch_data[:remaining_samples], batch_labels[:remaining_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iwe-uQguMlv2"
   },
   "source": [
    "Define train_source_path, Validation_Source_path and total number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693743110569,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "1DaWeI8_J_On",
    "outputId": "63b58881-fb2b-4ee9-be69-ce7000d78b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path=\"datasets/Project_data/train\"\n",
    "val_path=\"datasets/Project_data/val\"\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbcjSDW5NubW"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuuekm_bWTtm"
   },
   "source": [
    "#### 1) Using Conv3D and MaxPooling3D ( using without BatchNormalization and dropout )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1693743110569,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "pKpcq60yNzb4"
   },
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#   model = Sequential()\n",
    "\n",
    "#   model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=(15,120,120,1)))\n",
    "#   model.add(MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "#   model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "#   model.add(MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "#   model.add(Flatten())\n",
    "#   model.add(Dense(128, activation='relu'))\n",
    "#   model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Using Conv3D and MaxPooling3D along with dropout layers and batch-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1693743110570,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "LUe0DjAvvT55"
   },
   "outputs": [],
   "source": [
    "# # Using Conv3D and MaxPooling3D along with Dropout layer\n",
    "\n",
    "# def create_model():\n",
    "#   model = Sequential()\n",
    "\n",
    "#   model.add(Conv3D(16, (3, 3, 3), activation='relu', padding='same', input_shape=(15,120,120,1)))\n",
    "#   model.add(BatchNormalization())\n",
    "#   model.add(MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "#   model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same'))\n",
    "#   model.add(BatchNormalization())\n",
    "#   model.add(MaxPooling3D((2, 2, 2)))\n",
    "#   model.add(Dropout(0.25)) \n",
    "\n",
    "#   model.add(Flatten())  \n",
    "#   model.add(Dense(64, activation='relu'))\n",
    "#   model.add(Dropout(0.50))\n",
    "#   model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Using Conv3D and MaxPooling3D along with Dropout layers and Batch-Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using Conv3D and MaxPooling3D along with Dropout layer and Batch-Normalization layer\n",
    "\n",
    "# def create_model():\n",
    "#   model = Sequential()\n",
    "\n",
    "#   model.add(Conv3D(16, (3, 3, 3), activation='relu', padding='same', input_shape=(15,120,120,1)))\n",
    "#   model.add(BatchNormalization())\n",
    "#   model.add(MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "#   model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same'))\n",
    "#   model.add(BatchNormalization())\n",
    "#   model.add(MaxPooling3D((2, 2, 2)))\n",
    "#   model.add(Dropout(0.25)) \n",
    "    \n",
    "#   model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "#   model.add(BatchNormalization())\n",
    "#   model.add(MaxPooling3D((2, 2, 2)))\n",
    "#   model.add(Dropout(0.25))\n",
    "\n",
    "#   model.add(Flatten())\n",
    "\n",
    "#   model.add(Dense(128, activation='relu'))\n",
    "#   model.add(Dropout(0.50))\n",
    "#   model.add(Dense(64, activation='relu'))\n",
    "#   model.add(Dropout(0.50))\n",
    "#   model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Same as Model-3 but removed Batch-Normalization and Dropout layers from Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Conv3D and MaxPooling3D along with Dropout layer and Batch-Normalization layer\n",
    "\n",
    "def create_model():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Conv3D(16, (3, 3, 3), activation='relu', padding='same', input_shape=(15,120,120,1)))  \n",
    "  model.add(MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "  model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same')) \n",
    "  model.add(MaxPooling3D((2, 2, 2)))   \n",
    "    \n",
    "  model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))  \n",
    "  model.add(MaxPooling3D((2, 2, 2)))  \n",
    "\n",
    "  model.add(Flatten())\n",
    "\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dropout(0.50))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dropout(0.50))\n",
    "  model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Pre-trained MobileNet model with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#     # Load MobileNet pre-trained on ImageNet without the top classification layer\n",
    "#     base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(120, 120, 3))\n",
    "\n",
    "#     # Freeze the weights of the base model\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     # Define the model\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Add the MobileNetV2 base model with TimeDistributed wrapper\n",
    "#     model.add(TimeDistributed(base_model, input_shape=(15, 120, 120, 3)))\n",
    "\n",
    "#     # Flatten the output from the TimeDistributed layers\n",
    "#     model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#     # Recurrent layers (LSTM)\n",
    "#     model.add(LSTM(64))    \n",
    "\n",
    "#     # Fully connected layers\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Pre-trained MobileNet model with RNN (LSTM) with Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#     # Load MobileNet pre-trained on ImageNet without the top classification layer\n",
    "#     base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(120, 120, 3))\n",
    "\n",
    "#     # Freeze the weights of the base model\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     # Define the model\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Add the MobileNet base model with TimeDistributed wrapper\n",
    "#     model.add(TimeDistributed(base_model, input_shape=(15, 120, 120, 3)))\n",
    "\n",
    "#     # Flatten the output from the TimeDistributed layers\n",
    "#     model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#     # Recurrent layers (LSTM)\n",
    "#     model.add(LSTM(64))    \n",
    "\n",
    "#     # Fully connected layers\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Same as Model-6 but applied MaxPooling2D layer and used GRU instead of LSTM in RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#     # Load MobileNet pre-trained on ImageNet without the top classification layer\n",
    "#     base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(120, 120, 3))\n",
    "\n",
    "#     # Freeze the weights of the base model\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     # Define the model\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Add the MobileNet base model with TimeDistributed wrapper\n",
    "#     model.add(TimeDistributed(base_model, input_shape=(15, 120, 120, 3)))\n",
    "#     model.add(TimeDistributed(MaxPooling2D((2,2))))\n",
    "\n",
    "#     # Flatten the output from the TimeDistributed layers\n",
    "#     model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#     # Recurrent layers (GRU)\n",
    "#     model.add(GRU(32))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     # Fully connected layers\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3282,
     "status": "ok",
     "timestamp": 1693743113846,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "wehV38Bvb7l1",
    "outputId": "9aa15ebe-75ea-49b6-d4ee-fe5b7aa3da0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 17:18:54.530567: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-09-04 17:18:54.530632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14796 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:1e:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 15, 3, 3, 1024)   3228864   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 15, 1, 1, 1024)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 15, 1024)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                101568    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,332,869\n",
      "Trainable params: 104,005\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=create_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vv0ymLHdA0C"
   },
   "source": [
    "Let's create `train_generator` and `val_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1693743113846,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "zDGqHkWXc4Mv"
   },
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)  # For Model architecture 5,6 & 7 please use generator_2\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-IBIO8id8Pm"
   },
   "source": [
    "Let's define Model_checkpoint with appropriate Model name and also reduced learning rate on plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1693743113847,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "-GhcDTsedf9k"
   },
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "# Define the ReduceLROnPlateau callback and configure it with the desired parameters, such as the factor, patience, and min_lr. This callback will reduce the learning rate when a monitored metric (e.g., validation loss) plateaus.\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',   # Monitor validation loss\n",
    "    factor=0.5,           # Reduce learning rate by a factor of 0.5\n",
    "    patience=2,           # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,          # Minimum learning rate\n",
    "    verbose=1             # Provide verbose output\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvM6rJ-pnsqS"
   },
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1693743113848,
     "user": {
      "displayName": "Chintan Bhavsar",
      "userId": "12549939643751349714"
     },
     "user_tz": -330
    },
    "id": "gOOVBZ4pkwMC"
   },
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDGJXPxooNLy"
   },
   "source": [
    "Let's fit the Model and save at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Xbwk4Bvn0kd",
    "outputId": "891e8908-0ea5-4712-e628-8e497bd314d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  datasets/Project_data/train ; batch size = 32\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 17:19:02.430877: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 1.7388 - categorical_accuracy: 0.2368Source path =  datasets/Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54037, saving model to model_init_2023-09-0417_18_53.750742/model-00001-1.73880-0.23680-1.54037-0.30000.h5\n",
      "21/21 [==============================] - 88s 4s/step - loss: 1.7388 - categorical_accuracy: 0.2368 - val_loss: 1.5404 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6046 - categorical_accuracy: 0.2715\n",
      "Epoch 00002: val_loss improved from 1.54037 to 1.41272, saving model to model_init_2023-09-0417_18_53.750742/model-00002-1.60462-0.27149-1.41272-0.50000.h5\n",
      "21/21 [==============================] - 83s 4s/step - loss: 1.6046 - categorical_accuracy: 0.2715 - val_loss: 1.4127 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4709 - categorical_accuracy: 0.3635\n",
      "Epoch 00003: val_loss improved from 1.41272 to 1.28597, saving model to model_init_2023-09-0417_18_53.750742/model-00003-1.47094-0.36350-1.28597-0.70000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 1.4709 - categorical_accuracy: 0.3635 - val_loss: 1.2860 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3316 - categorical_accuracy: 0.4419\n",
      "Epoch 00004: val_loss improved from 1.28597 to 1.14174, saving model to model_init_2023-09-0417_18_53.750742/model-00004-1.33164-0.44193-1.14174-0.67000.h5\n",
      "21/21 [==============================] - 79s 4s/step - loss: 1.3316 - categorical_accuracy: 0.4419 - val_loss: 1.1417 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2423 - categorical_accuracy: 0.5038\n",
      "Epoch 00005: val_loss improved from 1.14174 to 1.05052, saving model to model_init_2023-09-0417_18_53.750742/model-00005-1.24231-0.50377-1.05052-0.67000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 1.2423 - categorical_accuracy: 0.5038 - val_loss: 1.0505 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1328 - categorical_accuracy: 0.5505\n",
      "Epoch 00006: val_loss improved from 1.05052 to 0.90353, saving model to model_init_2023-09-0417_18_53.750742/model-00006-1.13282-0.55053-0.90353-0.70000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 1.1328 - categorical_accuracy: 0.5505 - val_loss: 0.9035 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0175 - categorical_accuracy: 0.6124\n",
      "Epoch 00007: val_loss did not improve from 0.90353\n",
      "21/21 [==============================] - 82s 4s/step - loss: 1.0175 - categorical_accuracy: 0.6124 - val_loss: 0.9479 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9060 - categorical_accuracy: 0.6501\n",
      "Epoch 00008: val_loss improved from 0.90353 to 0.87608, saving model to model_init_2023-09-0417_18_53.750742/model-00008-0.90602-0.65008-0.87608-0.63000.h5\n",
      "21/21 [==============================] - 80s 4s/step - loss: 0.9060 - categorical_accuracy: 0.6501 - val_loss: 0.8761 - val_categorical_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7819 - categorical_accuracy: 0.7164\n",
      "Epoch 00009: val_loss improved from 0.87608 to 0.81386, saving model to model_init_2023-09-0417_18_53.750742/model-00009-0.78193-0.71644-0.81386-0.70000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 0.7819 - categorical_accuracy: 0.7164 - val_loss: 0.8139 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6888 - categorical_accuracy: 0.7526\n",
      "Epoch 00010: val_loss improved from 0.81386 to 0.64105, saving model to model_init_2023-09-0417_18_53.750742/model-00010-0.68877-0.75264-0.64105-0.74000.h5\n",
      "21/21 [==============================] - 82s 4s/step - loss: 0.6888 - categorical_accuracy: 0.7526 - val_loss: 0.6411 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5716 - categorical_accuracy: 0.7994\n",
      "Epoch 00011: val_loss did not improve from 0.64105\n",
      "21/21 [==============================] - 82s 4s/step - loss: 0.5716 - categorical_accuracy: 0.7994 - val_loss: 0.7855 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4940 - categorical_accuracy: 0.8250\n",
      "Epoch 00012: val_loss did not improve from 0.64105\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "21/21 [==============================] - 79s 4s/step - loss: 0.4940 - categorical_accuracy: 0.8250 - val_loss: 0.6558 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4857 - categorical_accuracy: 0.8401\n",
      "Epoch 00013: val_loss did not improve from 0.64105\n",
      "21/21 [==============================] - 83s 4s/step - loss: 0.4857 - categorical_accuracy: 0.8401 - val_loss: 0.6702 - val_categorical_accuracy: 0.7400 - lr: 5.0000e-04\n",
      "Epoch 14/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4323 - categorical_accuracy: 0.8507\n",
      "Epoch 00014: val_loss did not improve from 0.64105\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.4323 - categorical_accuracy: 0.8507 - val_loss: 0.7102 - val_categorical_accuracy: 0.7400 - lr: 5.0000e-04\n",
      "Epoch 15/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3893 - categorical_accuracy: 0.8582\n",
      "Epoch 00015: val_loss did not improve from 0.64105\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.3893 - categorical_accuracy: 0.8582 - val_loss: 0.7074 - val_categorical_accuracy: 0.7500 - lr: 2.5000e-04\n",
      "Epoch 16/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3884 - categorical_accuracy: 0.8899\n",
      "Epoch 00016: val_loss did not improve from 0.64105\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "21/21 [==============================] - 82s 4s/step - loss: 0.3884 - categorical_accuracy: 0.8899 - val_loss: 0.6475 - val_categorical_accuracy: 0.7400 - lr: 2.5000e-04\n",
      "Epoch 17/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3687 - categorical_accuracy: 0.8929\n",
      "Epoch 00017: val_loss did not improve from 0.64105\n",
      "21/21 [==============================] - 86s 4s/step - loss: 0.3687 - categorical_accuracy: 0.8929 - val_loss: 0.6706 - val_categorical_accuracy: 0.7500 - lr: 1.2500e-04\n",
      "Epoch 18/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3557 - categorical_accuracy: 0.8854\n",
      "Epoch 00018: val_loss improved from 0.64105 to 0.58328, saving model to model_init_2023-09-0417_18_53.750742/model-00018-0.35574-0.88537-0.58328-0.81000.h5\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.3557 - categorical_accuracy: 0.8854 - val_loss: 0.5833 - val_categorical_accuracy: 0.8100 - lr: 1.2500e-04\n",
      "Epoch 19/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3446 - categorical_accuracy: 0.8914\n",
      "Epoch 00019: val_loss did not improve from 0.58328\n",
      "21/21 [==============================] - 84s 4s/step - loss: 0.3446 - categorical_accuracy: 0.8914 - val_loss: 0.7481 - val_categorical_accuracy: 0.7200 - lr: 1.2500e-04\n",
      "Epoch 20/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3434 - categorical_accuracy: 0.9125\n",
      "Epoch 00020: val_loss did not improve from 0.58328\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "21/21 [==============================] - 81s 4s/step - loss: 0.3434 - categorical_accuracy: 0.9125 - val_loss: 0.6536 - val_categorical_accuracy: 0.7600 - lr: 1.2500e-04\n",
      "Epoch 21/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3684 - categorical_accuracy: 0.8854\n",
      "Epoch 00021: val_loss did not improve from 0.58328\n",
      "21/21 [==============================] - 81s 4s/step - loss: 0.3684 - categorical_accuracy: 0.8854 - val_loss: 0.6757 - val_categorical_accuracy: 0.7500 - lr: 6.2500e-05\n",
      "Epoch 22/30\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3373 - categorical_accuracy: 0.8884\n",
      "Epoch 00022: val_loss did not improve from 0.58328\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "21/21 [==============================] - 82s 4s/step - loss: 0.3373 - categorical_accuracy: 0.8884 - val_loss: 0.6722 - val_categorical_accuracy: 0.7600 - lr: 6.2500e-05\n",
      "Epoch 23/30\n",
      "15/21 [====================>.........] - ETA: 21s - loss: 0.3357 - categorical_accuracy: 0.8958"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1616/4120573176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1,\n",
    "                    callbacks=callbacks_list, validation_data=val_generator,\n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
